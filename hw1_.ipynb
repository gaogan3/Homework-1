{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: /user/st084310/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/user/st084310/proxy/4040/jobs/\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f41c18ee4f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sklearn\n",
    "import socket\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return \"{}proxy/{}/jobs/\".format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "# small fix to enable UI views\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# spark configurtion in local regime \n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '8g')\n",
    "\n",
    "#some needed objects\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "Transform text file \"The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelle\" into TF-IDF. Take row as \"document\".\n",
    "\n",
    "### Part 1: \n",
    "- read text file as dataframe \n",
    "- filter out non-letters and empty strings \n",
    "- transform into dataframe doc_id -> tf_idf vector \n",
    "\n",
    "\n",
    "### Part 2:\n",
    "- read text file as RDD\n",
    "- filter out non-letters and empty strings \n",
    "- transform into rdd in format doc_id -> tf_idf vector\n",
    "\n",
    "\n",
    "### Org part: \n",
    "I'm waiting your HW's as self-sufficient jupyter notebooks in github repository. \n",
    "\n",
    "Please, fill this table in specified comment with:\n",
    "\n",
    "your name / github link / telegram (optionally, if u want to discuss some) / \n",
    "\n",
    "Fill the comment please and i will add your data in a few days\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1p3yLsXqX2dp_TrPwNcikcS5FP_PTM0_gnSOzGn5Gn1E/edit#gid=0\n",
    "\n",
    "Feel free to text me if u have some questions \n",
    "\n",
    "### Deadline: 05.05.2021 included\n",
    "\n",
    "Dear students, dead in \"deadline\" means *dead*. This deadline is not for you - it's for me. Deadlines informs me from which point i should start to score your HWs.  You can commit anything after deadline but it's not guaranteed that I'll take it into account. It's possible to move deadline only for the whole group not \"just for me plz cause I was ill / detentioned / skipped this message\". \n",
    "\n",
    "### NB(!) \n",
    "\n",
    "It's not allowed to use TF-IDF code from Spark internal libraries. \n",
    "It's not allowed to cast DF/RDD into pandas and use scikit-learn. Please, keep it spark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Reading text file as rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                text| id|\n",
      "+--------------------+---+\n",
      "|The Project Guten...|  0|\n",
      "|                    |  1|\n",
      "|This eBook is for...|  2|\n",
      "|most other parts ...|  3|\n",
      "|whatsoever. You m...|  4|\n",
      "|of the Project Gu...|  5|\n",
      "|www.gutenberg.org...|  6|\n",
      "|will have to chec...|  7|\n",
      "|   using this eBook.|  8|\n",
      "|                    |  9|\n",
      "| Title: Frankenstein| 10|\n",
      "|       or, The Mo...| 11|\n",
      "|                    | 12|\n",
      "|Author: Mary Woll...| 13|\n",
      "|                    | 14|\n",
      "|Release Date: 31,...| 15|\n",
      "|[Most recently up...| 16|\n",
      "|                    | 17|\n",
      "|   Language: English| 18|\n",
      "|                    | 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_prefix = \"malyutin_demo_hw1\"\n",
    "\n",
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dataframe = sc.textFile(f\"{filepath}\")\\\n",
    "    .map(lambda x: (x,))\\\n",
    "    .toDF()\\\n",
    "    .select(F.col(\"_1\").alias(\"text\"))\\\n",
    "    .withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import string\n",
    "import re\n",
    "\n",
    "def process_string(data):\n",
    "\n",
    "    punct_removed = re.sub(r'[^\\w\\s]','',data)\n",
    "    words = punct_removed.lower().split(\" \")\n",
    "    \n",
    "    \n",
    "    return list(filter(lambda x: len(x) > 0, words))\n",
    "\n",
    "\n",
    "process_string_udf = udf(lambda z: process_string(z), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            by_words|\n",
      "+--------------------+\n",
      "|[the, project, gu...|\n",
      "|[this, ebook, is,...|\n",
      "|[most, other, par...|\n",
      "|[whatsoever, you,...|\n",
      "|[of, the, project...|\n",
      "|[wwwgutenbergorg,...|\n",
      "|[will, have, to, ...|\n",
      "|[using, this, ebook]|\n",
      "|[title, frankenst...|\n",
      "|[or, the, modern,...|\n",
      "|[author, mary, wo...|\n",
      "|[release, date, 3...|\n",
      "|[most, recently, ...|\n",
      "| [language, english]|\n",
      "|[character, set, ...|\n",
      "|[produced, by, ju...|\n",
      "|[further, correct...|\n",
      "|[start, of, the, ...|\n",
      "|[or, the, modern,...|\n",
      "|[by, mary, wollst...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "by_words = dataframe\\\n",
    "    .select(process_string_udf(F.col(\"text\")).alias(\"by_words\"))\\\n",
    "    .where(F.size(F.col(\"by_words\")) > 1)\n",
    "\n",
    "\n",
    "by_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the| 4377|\n",
      "|  and| 3038|\n",
      "|    i| 2840|\n",
      "|   of| 2762|\n",
      "|   to| 2169|\n",
      "|   my| 1773|\n",
      "|    a| 1441|\n",
      "|   in| 1187|\n",
      "| that| 1029|\n",
      "|  was| 1022|\n",
      "|   me|  861|\n",
      "| with|  713|\n",
      "|  but|  689|\n",
      "|  had|  686|\n",
      "|  you|  644|\n",
      "|   he|  604|\n",
      "|which|  565|\n",
      "|   it|  561|\n",
      "|   as|  536|\n",
      "|  not|  536|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_count = by_words.select(F.explode(F.col(\"by_words\")).alias(\"word\"))\\\n",
    "    .groupBy(F.col(\"word\")).count()\\\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "\n",
    "by_words_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_words_count.repartition(1)\\\n",
    "    .write.mode(\"overwrite\").csv(f\"df_by_words.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Reading text file into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_prefix = \"malyutin_demo_hw1\"\n",
    "\n",
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft (Godwin) Shelley',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere in the United States and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever. You may copy it, give it away or re-use it under the terms',\n",
       " 'of the Project Gutenberg License included with this eBook or online at',\n",
       " 'www.gutenberg.org. If you are not located in the United States, you',\n",
       " 'will have to check the laws of the country where you are located before',\n",
       " 'using this eBook.',\n",
       " '']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddText = sc.textFile(f\"{filepath}\").repartition(1)\n",
    "rddText.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordsRDD = rddText.flatMap(lambda line: line.lower().split(\" \")).filter(lambda line: len(line) > 1)\n",
    "wordCountRDD= wordsRDD.map(lambda word: (word, 1))\n",
    "\n",
    "resultRDD = wordCountRDD.reduceByKey(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4340),\n",
       " ('project', 83),\n",
       " ('gutenberg', 25),\n",
       " ('ebook', 8),\n",
       " ('of', 2759),\n",
       " ('frankenstein,', 8),\n",
       " ('by', 482),\n",
       " ('mary', 3),\n",
       " ('wollstonecraft', 3),\n",
       " ('(godwin)', 3)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# write it back to HDFS\n",
    "resultRDD.saveAsTextFile(f\"rdd_6\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
